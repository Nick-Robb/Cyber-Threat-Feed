import gspread
import requests
import pandas as pd
import random
from bs4 import BeautifulSoup
from datetime import datetime
from google.oauth2.service_account import Credentials

# Define authentication scopes
SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]

# Load Google Sheets credentials
CREDS = Credentials.from_service_account_file("credentials.json", scopes=SCOPES)
CLIENT = gspread.authorize(CREDS)

# Open Google Sheet
SHEET = CLIENT.open("Cyber Threat Intel Feed").sheet1  # Change if needed

# Rotate User-Agents to avoid being blocked
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36",
]

HEADERS = {"User-Agent": random.choice(USER_AGENTS)}

# -------------------------------
# ‚úÖ Function: Fetch Latest CVEs (Fixed API)
# -------------------------------
def fetch_latest_cves():
    url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
    response = requests.get(url, headers=HEADERS)
    
    print("\nüîπ Fetching CVE Data...")
    print(f"   üü¢ API Status Code: {response.status_code}")

    if response.status_code == 200:
        data = response.json()
        print(f"   üîç Number of CVEs Found: {len(data.get('vulnerabilities', []))}")
        
        cve_entries = []
        for item in data["vulnerabilities"][:5]:  # Fetch the latest 5 CVEs
            cve_id = item["cve"]["id"]
            description = item["cve"]["descriptions"][0]["value"]
            link = f"https://nvd.nist.gov/vuln/detail/{cve_id}"
            cve_entries.append([datetime.today().strftime('%Y-%m-%d'), "NVD", cve_id, "", description, link])
        
        return cve_entries
    
    print("‚ùå CVE API Failed - No Data Retrieved")
    return []

# -------------------------------
# ‚úÖ Function: Scrape Cybersecurity News (Fixed Selectors)
# -------------------------------
def fetch_cyber_news():
    sources = [
        ("BleepingComputer", "https://www.bleepingcomputer.com/news/security/", "h4.bc_latestnews a", "meta[name='description']", "time"),
        ("The Hacker News", "https://thehackernews.com/", "article header h2 a", "meta[name='description']", "span.story-date"),
        ("Dark Reading", "https://www.darkreading.com/", "section.article-listing h3 a", "meta[name='description']", "time"),
        ("Krebs on Security", "https://krebsonsecurity.com/", "h2.post-title a", "meta[name='description']", "time"),
    ]

    news_entries = []
    today_date = datetime.today().strftime('%Y-%m-%d')

    print("\nüîπ Fetching Cybersecurity News...")

    for source, base_url, selector, description_selector, date_selector in sources:
        print(f"üîé Attempting to connect to {source} ({base_url})...")

        try:
            response = requests.get(base_url, headers=HEADERS, timeout=10)
            print(f"   üü¢ {source} - Status Code: {response.status_code}")

            if response.status_code == 403:
                print(f"   ‚ùå {source} is blocking requests (403 Forbidden). Skipping...")
                continue

            soup = BeautifulSoup(response.text, "html.parser")
            articles = soup.select(selector)[:5]

            if not articles:
                print(f"   ‚ùå No Articles Found on {source}. Selector might be incorrect.")
                continue  
            
            for article in articles:
                title = article.get_text(strip=True)
                link = article.get("href")

                if link and not link.startswith("http"):
                    link = base_url + link

                # Fetch article page to extract description and date
                description = "No description available."
                publication_date = "Unknown"
                article_response = requests.get(link, headers=HEADERS, timeout=10)

                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.text, "html.parser")

                    # Extract description
                    if "meta" in description_selector:
                        meta_tag = article_soup.select_one(description_selector)
                        if meta_tag and meta_tag.get("content"):
                            description = meta_tag["content"]
                    else:
                        desc_tag = article_soup.select_one(description_selector)
                        if desc_tag:
                            description = desc_tag.get_text(strip=True)

                    # Extract publication date
                    date_tag = article_soup.select_one(date_selector)
                    if date_tag:
                        publication_date = date_tag.get_text(strip=True)

                # üîπ Print extracted data
                print(f"   ‚úÖ {source} | {title} | üìÖ {publication_date} | üîó {link}")

                # üîπ Append data in the correct format
                news_entries.append([
                    today_date,  # Date Added
                    source,  # News Source
                    title,  # Article Title
                    publication_date,  # Article Publish Date
                    description,  # Short Description
                    link  # Article Link
                ])

        except requests.exceptions.Timeout:
            print(f"   ‚ùå {source} request timed out. Skipping...")
        except requests.exceptions.RequestException as e:
            print(f"   ‚ùå Request error on {source}: {e}")
            continue  

    return news_entries

# -------------------------------
# ‚úÖ Function: Update Google Sheet
# -------------------------------
def update_google_sheet():
    cve_data = fetch_latest_cves()
    news_data = fetch_cyber_news()

    existing_data = SHEET.get_all_values()

    # üîπ Ensure the correct headers are set
    if len(existing_data) == 0:
        SHEET.append_row(["Date Added", "Source", "Title", "Publication Date", "Description", "Link"])

    all_data = []

    # üîπ Append CVE data
    for entry in cve_data:
        all_data.append(entry)  # (Date Added, Source, Title, Publication Date, Description, Link)

    # üîπ Append cybersecurity news data
    for entry in news_data:
        all_data.append(entry)

    # üîπ Add all rows at once (Better Performance)
    SHEET.append_rows(all_data)

    print("\n‚úÖ Cyber Threat Intel Feed Successfully Updated in Google Sheets!\n")


# Run the update function
update_google_sheet()
